VIT 이전 내용들도 공부하면 좋을 것 같아 RNN -> LSTM -> seq2seq -> attention -> transformer -> VIT 순서로 정리하였습니다(notion참조)

이중 RNN, LSTM은 직접 구현하였으며 VIT는 구현된 코드를 가져왔습니다. (모두 MNIST dataset 기반 학습)

seq2seq ~ trnasformer의 경우 cv dataset에 적용하기가 어려워 pass. (seq2seq는 시간되면 가능할 듯?)
